#  Copyright (c) 2017-2018 Uber Technologies, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

dist: xenial
language: python
cache: pip
python:
  - '2.7'
  - '3.6'

install:
  # Upgrade pip to avoid weird failures, such as failing to install torchvision
  - pip install --upgrade pip==18.0
  # This will use requirements from setup.py and install them in the tavis's virtual environment
  # [tf] chooses to depend on cpu version of tensorflow (alternatively, could do [tf_gpu])
  - pip install -q -e .[tf,test,torch,docs]
  # pyarrow was compiled against a newer version of numpy than we require so we need to upgrade it
  # (or optionally install pyarrow from source instead of through binaries)
  - pip install --upgrade numpy

before_script:
  # enable core dumps
  - ulimit -c unlimited -S

script:
  # Build documentation
  - (cd docs/autodoc && make html)

  - pylint --rcfile=.pylintrc petastorm examples -f parseable -r n
  # Ignore two pytorch tests to prevent static-TLS-caused torch-import seg-fault
  - pytest -v --cov=./ --trace-config
    --ignore=examples/mnist/tests/test_generate_mnist_dataset.py
    --ignore=petastorm/tests/test_pytorch_utils.py
  # Run the pytorch tests separately, in this order, but accumulate code coverage data
  # Temporary disabled until we figure out (a) segfaults in the first test with `import torch`,
  # and (b) hanging in `test_read_mnist_dataset`
  # - pytest -v examples/mnist/tests/test_generate_mnist_dataset.py petastorm/tests/test_pytorch_utils.py --cov=./ --cov-append

  # Verify caching of synthetic dataset is working (at least not failing)
  - pytest --cache-synthetic-dataset --log-cli-level info petastorm/tests/test_end_to_end.py::test_reading_subset_of_columns
  - pytest --cache-synthetic-dataset --log-cli-level info petastorm/tests/test_end_to_end.py::test_reading_subset_of_columns

after_success:
  - codecov

after_failure:
  # Only upon failure, install gdb to process core dump
  - sudo apt-get install -y gdb
  - COREFILE=$(find . -maxdepth 2 -name "core*" | head -n 1)
  # NOTE: without debug info, the stack trace won't be too useful.
  # If it's possible to run with python2.7-dbg, then gdb hook py-bt will do its magic.
  - if [[ -f "$COREFILE" ]]; then gdb -c "$COREFILE" python -ex "thread apply all bt" -ex "set pagination 0" -batch; fi

deploy:
  provider: pypi
  user: uber
  password:
    secure: gDx2UHI5mcBBAJCM71CBiyn6N2QF+HUx7+42BDc5uR3tL7Mk0fgNsl9tygG3hAabsujsG6eTotHGWUw3zRRN2+htyfQ4iiprivS+9LkPW/N9rp3IwcqfygoJk4ZTYRAnYAOtGJ+EWQU/8L8hekxAmuDKuWZfqO0gUSjxyKzE5bP9ly0HHk7pm201NdkoVUpl2DR/WCazbEuOTeHgNz72NirB00hkG7ScdIP1c6dnNqGe4UH7lbd6ZDHkTfvJAOR0BTTzGjP29e303AeHum++R47ddleUTgeiwBdhhgNhaSZY5IhWq32oNLWNd+j8mDpc1YaAb3AzWPS0CJvFPtH/bWyVUvlw343WinxjD5y4svlcoqxYzYtvKwAgO70Txdi9VLuc0F7FHz5CxcoP7L4OCf9nMA3v/0LK9P3dbBb4q9VK9pla6SyaHVR64BUHm/7+uPM8qPlZWV+NCiSrKy5R83zMpxDwXrxa9xXVKIHdrs7osmxMg5/AVMmIu/UVjs58LmLCSQWMYnpYJBgfycYzqA2fSXlelJxSL9wsDjBtkt9sZ0OUP2BQ2rJQAAJaLzHXduDAIiyvjFpZz5fC2B1HNIRKENxD6u2TFGUNuO7ubajiPpyOeXH6cNcrJdDLuhSVQXHYXtNqMuX0XBnKC7wPSmQdegjd8HUIRIij8XMAjkE=
  distributions: "sdist bdist_wheel"
  on:
    tags: true
    python: 3.6
