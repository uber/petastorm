#  Copyright (c) 2017-2018 Uber Technologies, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

dist: xenial
language: python
cache: pip
python:
  - '2.7'
  - '3.6'
matrix:
  include:
    - python: '3.7'
      sudo: required  # equired for Python 3.7 (travis-ci/travis-ci#9069)
  allow_failures:
    - python: '3.7'  # Until Python 3.7 support is added to https://pypi.org/project/pyarrow

install:
  # Upgrade pip to avoid weird failures, such as failing to install torchvision
  - pip install --upgrade pip==18.0
  # This will use requirements from setup.py and install them in the tavis's virtual environment
  # [tf] chooses to depend on cpu version of tensorflow (alternatively, could do [tf_gpu])
  - pip install -q -e .[tf,test,torch,docs]
  # pyarrow was compiled against a newer version of numpy than we require so we need to upgrade it
  # (or optionally install pyarrow from source instead of through binaries)
  - pip install --upgrade numpy

before_script:
  # enable core dumps
  - ulimit -c unlimited -S
  - pip install flake8
  # stop the build if there are Python syntax errors or undefined names
  - flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics
  # exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide
  - flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

script:
  # Build documentation
  - (cd docs/autodoc && make html)

  - pylint --rcfile=.pylintrc petastorm examples -f parseable -r n

  # Running pytest twice: first tests that do not have to be forked. Run it with -Y --cache-clear
  # which will create synthetic datasets that are used by multiple tests and store them in cache.
  # Next pytest invoke will run boxed tests, but read the synthetic datasets from cache and not
  # waste time on recreating them

  # Ignore two pytorch tests to prevent static-TLS-caused torch-import seg-fault
  - pytest -Y --cache-clear -m "not forked" -v --cov=./ --trace-config
    --ignore=examples/mnist/tests/test_generate_mnist_dataset.py
    --ignore=petastorm/tests/test_pytorch_utils.py petastorm examples

  # We have a separate run for forked: make sure fixtures are reused as much as possible
  - pytest -Y -m "forked" --forked -v --cov=./ --cov-append --trace-config
    --ignore=examples/mnist/tests/test_generate_mnist_dataset.py
    --ignore=petastorm/tests/test_pytorch_utils.py petastorm examples

  # Run the pytorch tests separately, in this order, but accumulate code coverage data
  # Temporary disabled until we figure out (a) segfaults in the first test with `import torch`,
  # and (b) hanging in `test_read_mnist_dataset`
  # - pytest -v examples/mnist/tests/test_generate_mnist_dataset.py petastorm/tests/test_pytorch_utils.py --cov=./ --cov-append

after_success:
  - codecov

after_failure:
  # Only upon failure, install gdb to process core dump
  - sudo apt-get install -y gdb
  - COREFILE=$(find . -maxdepth 2 -name "core*" | head -n 1)
  # NOTE: without debug info, the stack trace won't be too useful.
  # If it's possible to run with python2.7-dbg, then gdb hook py-bt will do its magic.
  - if [[ -f "$COREFILE" ]]; then gdb -c "$COREFILE" python -ex "thread apply all bt" -ex "set pagination 0" -batch; fi

before_deploy:
  # Build a python wheel before deployment
  - python setup.py bdist_wheel

deploy:
  # Disabling pypi deployment. There is an issue with authentication at the moment
  #  - provider: pypi
  #    user: uber
  #    password:
  #      secure: HYpHU5Gt6PnA/yEEX01zQBtJPwsToOpiL58INePElwUs8JvgniYs51hI19RpZeS7APju+9K6AyVdqW8wqkMrgULBDhIcdM/CI3XIXu6dsoa9MMdlGdtCsbBOXANbHCFuJbCTxkU828lvQsyQ/5y0pzMhC+C5AUV5qcH0zasOmXE7D61DIhnyCAteBAGGZRxR9EMHV6Q7gxG76Qg+4b70eiscK/98EtDVyRySL4J1LgbNsQiWOMu1eQ6Un4fMYsuR51vE7CpLk2ollFcT3Ivpt0LiutPDIJtZcf+VdSX9PFbUhMPHgEdj8LPDJe0/JMMT71/XyUQG2K+Dy2Xvzwt3h3p6UpyCyuPIZI9BngL4qmsyHcANu7q0kaWAUInPfZPtaqtT0qM7f9uyiZDPqHw5GoIGCxbKHiTpV2G8vd/XAU5Lqn386i05MKhR/J5VkQQHV5bmHNYBUXM6DFgeyGOdNHg5M2H+I7dla44VWn2PVJhAmZk0of/Wp8fv7lqe/FyHNyNpjtWa59rtY+4ZXSkpvhwLSaJPPRB93ObRPVw/AooVQtrWr+dgX2YTrzJcB0mvFbZse2vlM9MAoSv6bCiKCKdCEtb53ueGggFokT2LgfNLxuMO9ykuVH0Ko2Nzi/V1gD5sQ3BWlAFykPX3aAnGVvBBdB1WMMY2mO4Z+Wr6Tic=
  #    distributions: sdist bdist_wheel
  #    on:
  #      tags: true
  #      python: 3.6

  # Push all python wheels created to Github release
  - provider: releases
    skip_cleanup: true
    api_key:
      secure: I96u6KqgSj0Cqx2NvLLLgw6rx7+gN4/6wdptfBrP1zxppMERu/iaLYLgDwDKRXzgTIrx5LopFIllf9/kPcUJgDFj/AxESo6aukzcYK3tB2OhEYIAxYcZf8Kt3aV+AHp/TIDdZ9sGbVLxypuxSXqiAc5dJw8S0Njja/9Qgxe4jqNXNLjLxEb3qlCrYJFEA9MxxPgH/QsWZ6M43hR5gNrGWrhSaIHSjMXXkzRjQG+bGApFf+XRx7le2M5vIeaw1K/osJ930QEjDTnp0v5hFvkB9F1buC17rZO8Xy0KAAhB1+SaEWtS4N0lFIVYE1b3Ke1ek82kW1d7UDEAcEL6ccICboKtvhFfgb1MXVQkdRq4HlhwHl8n4FJIztbwXaQW1aISR5x+m6RIFwMuI+U/MjcVVt+CEgo8X4HATb2pdgWnFcS649t7lhrx7HCBNZpUNmfoZ/YyNgPdQf+vreCNxWOpqi4UacH890GhLFUFXtijeL7xGXWIg7ugQmlK27CfM1mMN1Vp9NTRAXA0x7HuqckWKY85lCJMT+3IJMgVjz+BdGZDQ5RJI22FmRDNuWrH/Mio77EK+sOsKm/X3Q8/Sb8uwU+Ft1Vb7aikQ2NIl89uQQdVxH4TlQw6/VQSZODacTUwMz5CkB0RJNrlbZpUpPrHiTVa4dRo7/34qe7iGA7XtfY=
    file_glob: true
    file: dist/*.whl
    on:
      tags: true
      python: 3.6
